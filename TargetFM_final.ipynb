{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/featurize/work/TargetFM/RNA-FM\n"
     ]
    }
   ],
   "source": [
    "cd /home/featurize/work/TargetFM/RNA-FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5834,
     "status": "ok",
     "timestamp": 1662708394106,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "45kifNVeJBQi",
    "outputId": "df9d544b-3fda-4b32-bafe-321ab5a10a88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Processing /home/featurize/work/TargetFM/RNA-FM\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Building wheels for collected packages: rna-fm\n",
      "  Building wheel for rna-fm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rna-fm: filename=rna_fm-0.1.0-py3-none-any.whl size=28798 sha256=2ca08f9be1fc38e5fd402e3fc584ad688733c31c5c5f09fb21457cdb18d18b29\n",
      "  Stored in directory: /home/featurize/.cache/pip/wheels/2d/f6/bf/5a73662ac34e4e6d77122ce63144d84a6b927d8c9c9057f669\n",
      "Successfully built rna-fm\n",
      "Installing collected packages: rna-fm\n",
      "  Attempting uninstall: rna-fm\n",
      "    Found existing installation: rna-fm 0.1.0\n",
      "    Uninstalling rna-fm-0.1.0:\n",
      "      Successfully uninstalled rna-fm-0.1.0\n",
      "Successfully installed rna-fm-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install . --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14900,
     "status": "ok",
     "timestamp": 1662708465609,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "6W5kLgU8h1Ba",
    "outputId": "0b2070b9-3f8b-4031-a1a0-107e4e9b98fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: biopython==1.68 in /home/featurize/work/.local/lib/python3.7/site-packages (1.68)\n"
     ]
    }
   ],
   "source": [
    "!pip install biopython==1.68 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: regex in /home/featurize/work/.local/lib/python3.7/site-packages (2022.8.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install regex --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/featurize/work/TargetFM\n"
     ]
    }
   ],
   "source": [
    "cd /home/featurize/work/TargetFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke1EP16f-uJm"
   },
   "source": [
    "# 1 Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1662708479894,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "xCGOHhw2S0ir"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import fm\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_rna\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1662708472967,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "TWZ15FJb9wcB"
   },
   "outputs": [],
   "source": [
    "\"\"\" 确定mRNA上的CTS片段位置 \"\"\"\n",
    "def find_candidate(mirna_sequence, mrna_sequence, seed_match):\n",
    "    positions = set()\n",
    "\n",
    "    # 确定seed_match方式\n",
    "    if seed_match == '10-mer-m6':\n",
    "        SEED_START = 1\n",
    "        SEED_END = 10\n",
    "        SEED_OFFSET = SEED_START - 1\n",
    "        MIN_MATCH = 6\n",
    "        TOLERANCE = (SEED_END-SEED_START+1) - MIN_MATCH\n",
    "    elif seed_match == '10-mer-m7':\n",
    "        SEED_START = 1\n",
    "        SEED_END = 10\n",
    "        SEED_OFFSET = SEED_START - 1\n",
    "        MIN_MATCH = 7\n",
    "        TOLERANCE = (SEED_END-SEED_START+1) - MIN_MATCH\n",
    "    elif seed_match == 'offset-9-mer-m7':\n",
    "        SEED_START = 2\n",
    "        SEED_END = 10\n",
    "        SEED_OFFSET = SEED_START - 1\n",
    "        MIN_MATCH = 7\n",
    "        TOLERANCE = (SEED_END-SEED_START+1) - MIN_MATCH\n",
    "    elif seed_match == 'strict':\n",
    "        positions = find_strict_candidate(mirna_sequence, mrna_sequence)\n",
    "        return positions\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"seed_match expected 'strict', '10-mer-m6', '10-mer-m7', or 'offset-9-mer-m7', got '{}'\".format(seed_match))\n",
    "    \n",
    "    # 确定mirna上对应seed区域\n",
    "    seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "\n",
    "    # complement()返回序列的转录序列； rc_seed:seed的配对片段\n",
    "    rc_seed = str(Seq(seed, generic_rna).complement())\n",
    "\n",
    "    # 在mrna中找可以与seed匹配的片段； re.finditer(pattern, string, flags=0), Use the finditer() function to match a pattern in a string and return an iterator yielding the Match objects.\n",
    "    match_iter = regex.finditer(\"({}){{e<={}}}\".format(rc_seed, TOLERANCE), mrna_sequence)\n",
    "\n",
    "    for match_index in match_iter:\n",
    "        # positions.add(match_index.start()) # slice-start indicies\n",
    "        positions.add(match_index.end()+SEED_OFFSET) # slice-stop indicies\n",
    "    \n",
    "    # CTS片段的第一个token位置\n",
    "    positions = list(positions)\n",
    "\n",
    "    return positions\n",
    "\n",
    "def find_strict_candidate(mirna_sequence, mrna_sequence):\n",
    "    positions = set()\n",
    "\n",
    "    SEED_TYPES = ['8-mer', '7-mer-m8', '7-mer-A1', '6-mer', '6-mer-A1', 'offset-7-mer', 'offset-6-mer']\n",
    "    for seed_match in SEED_TYPES:\n",
    "        if seed_match == '8-mer':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 8\n",
    "            SEED_OFFSET = 0\n",
    "            seed = 'U' + mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '7-mer-m8':\n",
    "            SEED_START = 1\n",
    "            SEED_END = 8\n",
    "            SEED_OFFSET = 0\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '7-mer-A1':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 7\n",
    "            SEED_OFFSET = 0\n",
    "            seed = 'U' + mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '6-mer':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 7\n",
    "            SEED_OFFSET = 1\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == '6mer-A1':\n",
    "            SEED_START = 2\n",
    "            SEED_END = 6\n",
    "            SEED_OFFSET = 0\n",
    "            seed = 'U' + mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == 'offset-7-mer':\n",
    "            SEED_START = 3\n",
    "            SEED_END = 9\n",
    "            SEED_OFFSET = 0\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "        elif seed_match == 'offset-6-mer':\n",
    "            SEED_START = 3\n",
    "            SEED_END = 8\n",
    "            SEED_OFFSET = 0\n",
    "            seed = mirna_sequence[(SEED_START-1):SEED_END]\n",
    "\n",
    "        rc_seed = str(Seq(seed, generic_rna).complement())\n",
    "        match_iter = regex.finditer(rc_seed, mrna_sequence)\n",
    "\n",
    "        for match_index in match_iter:\n",
    "            # positions.add(match_index.start()) # slice-start indicies\n",
    "            positions.add(match_index.end()+SEED_OFFSET) # slice-stop indicies\n",
    "\n",
    "    positions = list(positions)\n",
    "\n",
    "    return positions\n",
    "\n",
    "\"\"\" 确定CTS片段 \"\"\"\n",
    "def get_candidate(mirna_sequence, mrna_sequence, cts_size, seed_match):\n",
    "    positions = find_candidate(mirna_sequence, mrna_sequence, seed_match)  # CTS片段首位置（list）\n",
    "\n",
    "    candidates = []\n",
    "    for i in positions:\n",
    "        site_sequence = mrna_sequence[max(0, i-cts_size):i]      # mrna上CTS片段\n",
    "        rev_site_sequence = site_sequence[::-1]                  # [::-1]序列翻转：从左到右->从右到左\n",
    "        rc_site_sequence = str(Seq(rev_site_sequence, generic_rna).complement())   # 转录序列\n",
    "        candidates.append(rev_site_sequence)                     # miRNAs: 5'-ends to 3'-ends,  mRNAs: 3'-ends to 5'-ends\n",
    "        #candidates.append(rc_site_sequence)\n",
    "\n",
    "    return candidates, positions\n",
    "\n",
    "\"\"\" 生成pairs \"\"\"\n",
    "def make_pair(mirna_sequence, mrna_sequence, cts_size, seed_match):\n",
    "    candidates, positions = get_candidate(mirna_sequence, mrna_sequence, cts_size, seed_match) # candidates:list of seq\n",
    "\n",
    "    mirna_querys = []     # mirna seed region\n",
    "    mrna_targets = []     # mirna CTS\n",
    "    if len(candidates) == 0:   # mrna中无mirna匹配片段\n",
    "        return (mirna_querys, mrna_targets, positions)\n",
    "    else: \n",
    "        mirna_sequence = mirna_sequence[0:cts_size]   # mirna的seed region\n",
    "        for i in range(len(candidates)):\n",
    "            mirna_querys.append(mirna_sequence)\n",
    "            mrna_targets.append(candidates[i])        # 一个mirna的seed对应多个mrna的CTS\n",
    "\n",
    "    return mirna_querys, mrna_targets, positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1662708496501,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "tiXCuu9P7ozS"
   },
   "outputs": [],
   "source": [
    "\"\"\" 读取mirna,mrna序列 \"\"\"\n",
    "def read_fasta(mirna_fasta_file, mrna_fasta_file):\n",
    "    mirna_list = list(SeqIO.parse(mirna_fasta_file, 'fasta'))\n",
    "    mrna_list = list(SeqIO.parse(mrna_fasta_file, 'fasta'))\n",
    "\n",
    "    mirna_ids = []\n",
    "    mirna_seqs = []\n",
    "    mrna_ids = []\n",
    "    mrna_seqs = []\n",
    "\n",
    "    for i in range(len(mirna_list)):\n",
    "        mirna_ids.append(str(mirna_list[i].id))\n",
    "        mirna_seqs.append(str(mirna_list[i].seq))\n",
    "\n",
    "    for i in range(len(mrna_list)):\n",
    "        mrna_ids.append(str(mrna_list[i].id))\n",
    "        mrna_seqs.append(str(mrna_list[i].seq))\n",
    "\n",
    "    return mirna_ids, mirna_seqs, mrna_ids, mrna_seqs\n",
    "\n",
    "\"\"\" 读取gt(label) \"\"\"\n",
    "def read_ground_truth(ground_truth_file, header=True, train=False):\n",
    "    # input format: [MIRNA_ID, MRNA_ID, LABEL]\n",
    "    if header is True:\n",
    "        records = pd.read_csv(ground_truth_file, header=0, sep='\\t')\n",
    "    else:\n",
    "        records = pd.read_csv(ground_truth_file, header=None, sep='\\t')\n",
    "\n",
    "    query_ids = np.asarray(records.iloc[:, 0].values)\n",
    "    target_ids = np.asarray(records.iloc[:, 1].values)\n",
    "    if train is True:\n",
    "        labels = np.asarray(records.iloc[:, 2].values)\n",
    "    else:\n",
    "        labels = np.full((len(records),), fill_value=-1)\n",
    "\n",
    "    return query_ids, target_ids, labels\n",
    "\n",
    "\n",
    "\"\"\" 核苷酸转整型 \"\"\"\n",
    "# AUUCAAU -> 1442114\n",
    "def nucleotide_to_int(nucleotides, max_len):\n",
    "    dictionary = {'A':1, 'C':2, 'G':3, 'T':4, 'U':4}\n",
    "\n",
    "    chars = []\n",
    "    nucleotides = nucleotides.upper()   # nucleotides小写转大写\n",
    "    for c in nucleotides:\n",
    "        chars.append(c)\n",
    "\n",
    "    ints_enc = np.full((max_len,), fill_value=0) # to post-pad inputs; np.full(shape, fill_value)返回一个给定大小和类型并且以指定数字全部填充的新数组\n",
    "    for i in range(len(chars)):\n",
    "        try:\n",
    "            ints_enc[i] = dictionary[chars[i]]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except IndexError:\n",
    "            break\n",
    "\n",
    "    return ints_enc\n",
    "\n",
    "\"\"\" 序列转整型 \"\"\"\n",
    "# 所有序列转整型\n",
    "def sequence_to_int(sequences, max_len):\n",
    "    import itertools\n",
    "\n",
    "    if type(sequences) is list:\n",
    "        seqs_enc = np.asarray([nucleotide_to_int(seq, max_len) for seq in sequences])\n",
    "    else:\n",
    "        seqs_enc = np.asarray([nucleotide_to_int(seq, max_len) for seq in sequences])\n",
    "        seqs_enc = list(itertools.chain(*seqs_enc))\n",
    "        seqs_enc = np.asarray(seqs_enc)\n",
    "\n",
    "    return seqs_enc\n",
    "\n",
    "\"\"\" 统一序列长度，两种pad方式 \"\"\"\n",
    "def pad_sequences(sequences, max_len=None, padding='pre', fill_value='O'):\n",
    "    n_samples = len(sequences)    # 样本数：序列个数； sequences:list of sequence\n",
    "\n",
    "    lengths = []\n",
    "    for seq in sequences:\n",
    "        try:\n",
    "            lengths.append(len(seq))  # 记录每一个序列的长度\n",
    "        except TypeError:\n",
    "            raise ValueError(\"sequences expected a list of iterables, got {}\".format(seq))\n",
    "    if max_len is None:\n",
    "        max_len = np.max(lengths)  # 确定最大序列长度\n",
    "\n",
    "    # input_shape = np.asarray(sequences[0]).shape[1:]   # ???\n",
    "    # padded_shape = (n_samples, max_len) + input_shape\n",
    "    # padded = np.full(padded_shape, fill_value=fill_value)\n",
    "    \n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if padding == 'pre':\n",
    "            if max_len > len(seq):\n",
    "                sequences[i] = [fill_value]*(max_len - len(seq)) + sequences[i]\n",
    "            else:\n",
    "                sequences[i] = sequences[i][:max_len]\n",
    "        elif padding == 'post':\n",
    "            if max_len > len(seq):\n",
    "                sequences[i] = sequences[i] + [fill_value]*(max_len - len(seq))\n",
    "            else:\n",
    "                sequences[i] = sequences[i][:max_len]\n",
    "        else:\n",
    "            raise ValueError(\"padding expected 'pre' or 'post', got {}\".format(truncating))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "\"\"\" 对label进行编码 （samples, classes）\"\"\"\n",
    "def to_categorical(labels, n_classes=None):\n",
    "    labels = np.array(labels, dtype='int').reshape(-1)   \n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    if not n_classes:\n",
    "        n_classes = np.max(labels) + 1\n",
    "\n",
    "    categorical = np.zeros((n_samples, n_classes))\n",
    "    categorical[np.arange(n_samples), labels] = 1\n",
    "\n",
    "    return categorical\n",
    "\n",
    "\n",
    "\"\"\" 对miran,mrna,y进行one-hot编码 \"\"\"\n",
    "def preprocess_data(x_query_seqs, x_target_seqs, y=None, cts_size=None, pre_padding=False):\n",
    "    if cts_size is not None:\n",
    "        max_len = cts_size\n",
    "    else:\n",
    "        max_len = max(len(max(x_query_seqs, key=len)), len(max(x_target_seqs, key=len)))\n",
    "    \n",
    "    # 将mirna,mran转为整型\n",
    "    # x_mirna = sequence_to_int(x_query_seqs, max_len)\n",
    "    # x_mrna = sequence_to_int(x_target_seqs, max_len)\n",
    " \n",
    "    # padding, max取max(mirna,mrna)\n",
    "    # if pre_padding:\n",
    "    x_query_seqs = [list(i) for i in x_query_seqs]\n",
    "    x_target_seqs = [list(i) for i in x_target_seqs]\n",
    "    x_mirna = pad_sequences(x_query_seqs, max_len, padding='pre')\n",
    "    x_mrna = pad_sequences(x_target_seqs, max_len, padding='pre')\n",
    "    \n",
    "    # 对mrna,mirna进行one-hot编码\n",
    "    # x_mirna_embd = one_hot_enc(x_mirna)\n",
    "    # x_mrna_embd = one_hot_enc(x_mrna)\n",
    "    if y is not None:\n",
    "        y_embd = to_categorical(y, np.unique(y).size)  \n",
    "\n",
    "        return x_mirna, x_mrna, y_embd\n",
    "    else:\n",
    "        return x_mirna, x_mrna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1662708513865,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "nb4NW5vK9fyz"
   },
   "outputs": [],
   "source": [
    "\"\"\" 构造dataset(字典) \"\"\"\n",
    "def make_input_pair(mirna_fasta_file, mrna_fasta_file, ground_truth_file, cts_size=30, seed_match='offset-9-mer-m7', header=True, train=True):\n",
    "    mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file, mrna_fasta_file)           # mirna_ids, mirna_seqs, mrna_ids, mrna_seqs\n",
    "    query_ids, target_ids, labels = read_ground_truth(ground_truth_file, header=header, train=train)     # query_ids, target_ids, labels\n",
    "\n",
    "    dataset = {\n",
    "        'mirna_fasta_file': mirna_fasta_file,\n",
    "        'mrna_fasta_file': mrna_fasta_file,\n",
    "        'ground_truth_file': ground_truth_file,\n",
    "        'query_ids': [],\n",
    "        'query_seqs': [],\n",
    "        'target_ids': [],\n",
    "        'target_seqs': [],\n",
    "        'target_locs': [],\n",
    "        'labels': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(query_ids)):\n",
    "        try:\n",
    "            j = mirna_ids.index(query_ids[i])    # j:mirna index\n",
    "        except ValueError:       \n",
    "            continue\n",
    "        try:\n",
    "            k = mrna_ids.index(target_ids[i])    # k:mrna index\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        query_seqs, target_seqs, locations = make_pair(mirna_seqs[j], mrna_seqs[k], cts_size=cts_size, seed_match=seed_match)\n",
    "\n",
    "        n_pairs = len(locations)    # 产生的 mirna-mrna匹配对数\n",
    "        if n_pairs > 0:\n",
    "            queries = [query_ids[i] for n in range(n_pairs)]\n",
    "            dataset['query_ids'].extend(queries)\n",
    "            dataset['query_seqs'].extend(query_seqs)\n",
    "\n",
    "            targets = [target_ids[i] for n in range(n_pairs)]\n",
    "            dataset['target_ids'].extend(targets)\n",
    "            dataset['target_seqs'].extend(target_seqs)\n",
    "            dataset['target_locs'].extend(locations)\n",
    "\n",
    "            dataset['labels'].extend([[labels[i]] for p in range(n_pairs)])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\"\"\" 无labels \"\"\"\n",
    "def make_brute_force_pair(mirna_fasta_file, mrna_fasta_file, cts_size=30, seed_match='offset-9-mer-m7'):\n",
    "    mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file, mrna_fasta_file)\n",
    "\n",
    "    dataset = {\n",
    "        'query_ids': [],\n",
    "        'query_seqs': [],\n",
    "        'target_ids': [],\n",
    "        'target_seqs': [],\n",
    "        'target_locs': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(mirna_ids)):\n",
    "        for j in range(len(mrna_ids)):\n",
    "            query_seqs, target_seqs, positions = make_pair(mirna_seqs[i], mrna_seqs[j], cts_size, seed_match)\n",
    "\n",
    "            n_pairs = len(positions)\n",
    "            if n_pairs > 0:\n",
    "                query_ids = [mirna_ids[i] for k in range(n_pairs)]\n",
    "                dataset['query_ids'].extend(query_ids)\n",
    "                dataset['query_seqs'].extend(query_seqs)\n",
    "\n",
    "                target_ids = [mrna_ids[j] for k in range(n_pairs)]\n",
    "                dataset['target_ids'].extend(target_ids)\n",
    "                dataset['target_seqs'].extend(target_seqs)\n",
    "                dataset['target_locs'].extend(positions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1662708527047,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "8XkAxFHM70UA"
   },
   "outputs": [],
   "source": [
    "\"\"\" 生成负样本 \"\"\"\n",
    "def get_negative_pair(mirna_fasta_file, mrna_fasta_file, ground_truth_file=None, cts_size=30, seed_match='offset-9-mer-m7', header=False, predict_mode=True):\n",
    "    mirna_ids, mirna_seqs, mrna_ids, mrna_seqs = read_fasta(mirna_fasta_file, mrna_fasta_file)   # 读取mrna,mirna文件\n",
    "  \n",
    "    dataset = {\n",
    "        'query_ids': [],\n",
    "        'target_ids': [],\n",
    "        'predicts': []\n",
    "    }\n",
    "\n",
    "    if ground_truth_file is not None:\n",
    "        query_ids, target_ids, labels = read_ground_truth(ground_truth_file, header=header)\n",
    "\n",
    "        for i in range(len(query_ids)):\n",
    "            try:\n",
    "                j = mirna_ids.index(query_ids[i])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            try:\n",
    "                k = mrna_ids.index(target_ids[i])\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            query_seqs, target_seqs, locations = make_pair(mirna_seqs[j], mrna_seqs[k], cts_size=cts_size, seed_match=seed_match)\n",
    "\n",
    "            n_pairs = len(locations)\n",
    "            if (n_pairs == 0) and (predict_mode is True):\n",
    "                dataset['query_ids'].append(query_ids[i])\n",
    "                dataset['target_ids'].append(target_ids[i])\n",
    "                dataset['predicts'].append(0)\n",
    "            elif (n_pairs == 0) and (predict_mode is False):\n",
    "                dataset['query_ids'].append(query_ids[i])\n",
    "                dataset['target_ids'].append(target_ids[i])\n",
    "                dataset['predicts'].append(labels[i])\n",
    "    else:\n",
    "        for i in range(len(mirna_ids)):\n",
    "            for j in range(len(mrna_ids)):\n",
    "                query_seqs, target_seqs, locations = make_pair(mirna_seqs[i], mrna_seqs[j], cts_size=cts_size, seed_match=seed_match)\n",
    "\n",
    "                n_pairs = len(locations)\n",
    "                if n_pairs == 0:\n",
    "                    dataset['query_ids'].append(mirna_ids[i])\n",
    "                    dataset['target_ids'].append(mrna_ids[j])\n",
    "                    dataset['predicts'].append(0)\n",
    "\n",
    "    dataset['target_locs'] = [-1 for i in range(len(dataset['query_ids']))]\n",
    "    dataset['probabilities'] = [0.0 for i in range(len(dataset['query_ids']))]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\"\"\" 结果统计 \"\"\"\n",
    "def postprocess_result(dataset, probabilities, predicts, predict_mode=True, output_file=None, cts_size=30, seed_match='offset-9-mer-m7', level='site'):\n",
    "    neg_pairs = get_negative_pair(dataset['mirna_fasta_file'], dataset['mrna_fasta_file'], dataset['ground_truth_file'], cts_size=cts_size, seed_match=seed_match, predict_mode=predict_mode)   # 负样本对\n",
    "\n",
    "    # dataset:正样本集  neg_pair:负样本集\n",
    "    query_ids = np.append(dataset['query_ids'], neg_pairs['query_ids'])         \n",
    "    target_ids = np.append(dataset['target_ids'], neg_pairs['target_ids'])\n",
    "    target_locs = np.append(dataset['target_locs'], neg_pairs['target_locs'])\n",
    "    probabilities = np.append(probabilities, neg_pairs['probabilities'])        # probabilities：正样本训练经模型得到的prob  neg_pairs['probabilities]：构造负样本时设定好的prob=0.0\n",
    "    predicts = np.append(predicts, neg_pairs['predicts'])                       # predicts:正样本训练经模型得到的预测   neg_pairs['predicts']：构造负样本时设定好的prob=-1\n",
    "\n",
    "    # output format: [QUERY, TARGET, LOCATION, PROBABILITY]\n",
    "    records = pd.DataFrame(columns=['MIRNA_ID', 'MRNA_ID', 'LOCATION', 'PROBABILITY'])\n",
    "    records['MIRNA_ID'] = query_ids\n",
    "    records['MRNA_ID'] = target_ids\n",
    "    records['LOCATION'] = np.array([\"{},{}\".format(max(1, l-cts_size+1), l) if l != -1 else \"-1,-1\" for l in target_locs])\n",
    "    records['PROBABILITY'] = probabilities\n",
    "    if predict_mode is True:                  # 是否在预测\n",
    "        records['PREDICT'] = predicts\n",
    "    else:\n",
    "        records['LABEL'] = predicts\n",
    "\n",
    "    # site level\n",
    "    records = records.sort_values(by=['PROBABILITY', 'MIRNA_ID', 'MRNA_ID'], ascending=[False, True, True])  # sort_values()函数原理类似于SQL中的order by，将数据集依照某个字段中的数据进行排序； ascending\t是否按指定列的数组升序排列，默认为True，即升序排列\n",
    "    # gene level\n",
    "    unique_records = records.sort_values(by=['PROBABILITY', 'MIRNA_ID', 'MRNA_ID'], ascending=[False, True, True]).drop_duplicates(subset=['MIRNA_ID', 'MRNA_ID'], keep='first')\n",
    "\n",
    "    if level == 'site':\n",
    "        if output_file is not None:\n",
    "            records.to_csv(output_file, index=False, sep='\\t') \n",
    "        return records\n",
    "\n",
    "    elif level == 'gene':\n",
    "        if output_file is not None:\n",
    "            unique_records.to_csv(output_file, index=False, sep='\\t')\n",
    "        return unique_records\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"level expected 'site' or 'gene', got '{}'\".format(mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jrIssvoHrMp"
   },
   "source": [
    "# 2 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1662708402068,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "ekvQ-ae-Hzjv"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mirna_fasta_file, mrna_fasta_file, ground_truth_file, cts_size=30, seed_match='offset-9-mer-m7', header=True, train=True):\n",
    "        self.dataset = make_input_pair(mirna_fasta_file, mrna_fasta_file, ground_truth_file, cts_size=cts_size, seed_match=seed_match, header=header, train=train)  # 'query_ids': [], 'query_seqs': [], 'target_ids': [], 'target_seqs': [], 'target_locs': [], 'labels': []\n",
    "        self.mirna, self.mrna = preprocess_data(self.dataset['query_seqs'], self.dataset['target_seqs'])   # x_mirna_embd, x_mrna_embd, y_embd\n",
    "        self.labels = np.asarray(self.dataset['labels']).reshape(-1,)\n",
    "        \n",
    "        # self.mirna = self.mirna.transpose((0, 2, 1))\n",
    "        # self.mrna = self.mrna.transpose((0, 2, 1))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        mirna = self.mirna[index]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([('1', ''.join(mirna))])\n",
    "        mirna = batch_tokens[0][1:-1]\n",
    "        mrna = self.mrna[index]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([('1', ''.join(mrna))])\n",
    "        mrna = batch_tokens[0][1:-1]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return (mirna, mrna), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ground_truth_file, cts_size=30):\n",
    "        self.records = pd.read_csv(ground_truth_file, header=0, sep='\\t')\n",
    "        mirna_seqs = self.records['MIRNA_SEQ'].values.tolist()\n",
    "        mrna_seqs = self.records['MRNA_SEQ'].values.tolist()\n",
    "        self.mirna, self.mrna = preprocess_data(mirna_seqs, mrna_seqs, cts_size=cts_size)\n",
    "        self.labels = self.records['LABEL'].values.astype(int)\n",
    "        \n",
    "        # self.mirna = self.mirna.transpose((0, 2, 1))\n",
    "        # self.mrna = self.mrna.transpose((0, 2, 1))\n",
    "        # batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        \n",
    "        mirna = self.mirna[index]\n",
    "        mrna = self.mrna[index]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([('1', ''.join(mirna))])\n",
    "        mirna = batch_tokens[0][1:-1]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter([('1', ''.join(mrna))])\n",
    "        mrna = batch_tokens[0][1:-1]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        return (mirna, mrna), label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD3gNIepLfBP",
    "tags": []
   },
   "source": [
    "# 3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD3gNIepLfBP",
    "tags": []
   },
   "source": [
    "## RNA-FM Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p /home/featurize/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /home/featurize/work/TargetFM/RNA-FM_pretrained.pth /home/featurize/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "34066fc389e74abda6cc481fe7f744d9",
      "7e731091a93849edad827ce94a85e7d4",
      "f96c7ee0c6254068a9ee9a212cc173f9",
      "4b8235dcebeb46ed8432bc0a78ebc9e8",
      "42abe6644ea9477d9b690397910d5d7a",
      "1a3d4d3e01704a3993da42e97dd2243d",
      "f5b725c12057456aae21366a65ae6226",
      "ee093b64aa134309b09faabcd714dc72",
      "0f9637eb787249aeb29f655aeecf0b1f",
      "f7467ca42b614cc7b3e9967fadb18e8c",
      "be95e77ac42348a6bc912f6144f89efa"
     ]
    },
    "executionInfo": {
     "elapsed": 712124,
     "status": "ok",
     "timestamp": 1662709303485,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "bzU1PkL87Xfe",
    "outputId": "7a12eca5-e954-44f9-c458-58b0caf8eb4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNABertModel(\n",
       "  (embed_tokens): Embedding(25, 640, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (8): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (9): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (10): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (11): TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=5120, bias=True)\n",
       "      (fc2): Linear(in_features=5120, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=240, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (embed_positions): LearnedPositionalEmbedding(1026, 640, padding_idx=1)\n",
       "  (emb_layer_norm_before): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import fm\n",
    "\n",
    "# Load RNA-FM model\n",
    "fm_model, alphabet = fm.pretrained.rna_fm_t12()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "fm_model = fm_model.to(device)\n",
    "fm_model.eval()  # disables dropout for deterministic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1662708407894,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "by26Yhbj7Xff"
   },
   "outputs": [],
   "source": [
    "def get_embed(batch_tokens):\n",
    "    # batch_tokens =batch_tokens.squeeze(0)\n",
    "    with torch.no_grad():\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "        results = fm_model(batch_tokens, repr_layers=[12])\n",
    "    token_embeddings = results[\"representations\"][12]\n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1662708410573,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "CfsVHIeznyz5"
   },
   "outputs": [],
   "source": [
    "\"\"\" 网络超参设置, filters/kernel \"\"\"\n",
    "class HyperParam:\n",
    "    def __init__(self, filters=None, kernels=None, model_json=None):\n",
    "        self.dictionary = dict()\n",
    "        self.name_postfix = str()\n",
    "\n",
    "        if (filters is not None) and (kernels is not None) and (model_json is None):\n",
    "            for i, (f, k) in enumerate(zip(filters, kernels)):     # get the elements of multiple lists and indexes https://note.nkmk.me/en/python-for-enumerate-zip/\n",
    "                setattr(self, 'f{}'.format(i+1), f)\n",
    "                setattr(self, 'k{}'.format(i+1), k)\n",
    "                self.dictionary.update({'f{}'.format(i+1): f, 'k{}'.format(i+1): k})\n",
    "            self.len = i+1\n",
    "                \n",
    "            for key, value in self.dictionary.items():\n",
    "                self.name_postfix = \"{}_{}-{}\".format(self.name_postfix, key, value)\n",
    "        elif model_json is not None:\n",
    "            self.dictionary = json.loads(model_json\n",
    "                                        )\n",
    "            for i, (key, value) in enumerate(self.dictionary.items()):\n",
    "                setattr(self, key, value)\n",
    "                self.name_postfix = \"{}_{}-{}\".format(self.name_postfix, key, value)\n",
    "            self.len = (i+1)//2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662708411052,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "tNpagN1erkU3"
   },
   "outputs": [],
   "source": [
    "class TargetFM1(nn.Module):\n",
    "    def __init__(self, hparams=None, hidden_units=30, input_shape=(2, 30), name_prefix=\"model\"):\n",
    "        super(TargetFM1, self).__init__()\n",
    "\n",
    "        if hparams is None:\n",
    "            filters, kernels = [32, 16, 64, 16], [3, 3, 3, 3]\n",
    "            hparams = HyperParam(filters, kernels)\n",
    "        self.name = \"{}{}\".format(name_prefix, hparams.name_postfix)\n",
    "        self.fc_mi = nn.Linear(640, 32)\n",
    "        self.fc_mr = nn.Linear(640, 32)\n",
    "        \n",
    "        if (isinstance(hparams, HyperParam)) and (len(hparams) == 4):\n",
    "            self.embd1 = nn.Conv1d(4, hparams.f1, kernel_size=hparams.k1, padding=((hparams.k1 - 1) // 2))\n",
    "            \n",
    "            self.conv2 = nn.Conv1d(hparams.f1*2, hparams.f2, kernel_size=hparams.k2)\n",
    "            self.conv3 = nn.Conv1d(hparams.f2, hparams.f3, kernel_size=hparams.k3)\n",
    "            self.conv4 = nn.Conv1d(hparams.f3, hparams.f4, kernel_size=hparams.k4)\n",
    "            \n",
    "            \"\"\" out_features = ((in_length - kernel_size + (2 * padding)) / stride + 1) * out_channels \"\"\"\n",
    "            # flat_features = self.forward(torch.randint(1, 5, input_shape).to(device), torch.randint(1, 5, input_shape).to(device), flat_check=False)\n",
    "            self.fc1 = nn.Linear(384, hidden_units)\n",
    "            self.fc2 = nn.Linear(hidden_units, 2)\n",
    "        else:\n",
    "            raise ValueError(\"not enough hyperparameters\")\n",
    "    \n",
    "    def forward(self, x_mirna, x_mrna, flat_check=False):\n",
    "        mi_out = get_embed(x_mirna)\n",
    "        mi_out = self.fc_mi(mi_out).transpose(1,2)\n",
    "        mr_out = get_embed(x_mrna)\n",
    "        mr_out = self.fc_mr(mr_out).transpose(1,2)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        h_mirna = F.relu(mi_out)                   # torch.Size([32, 32, 30])\n",
    "        # print(h_mirna.shape)\n",
    "        h_mrna = F.relu(mr_out)                    # torch.Size([32, 32, 30])\n",
    "        # print(h_mrna.shape)\n",
    "        h = torch.cat((h_mirna, h_mrna), dim=1)    # torch.Size([32, 64, 30])\n",
    "        # print(h.shape)\n",
    "        h = F.relu(self.conv2(h))                  # torch.Size([32, 16, 28])\n",
    "        # print(h.shape)\n",
    "        h = F.relu(self.conv3(h))                  # torch.Size([32, 64, 26])\n",
    "        # print(h.shape)\n",
    "        h = F.relu(self.conv4(h))                  # torch.Size([32, 16, 24])\n",
    "        # print(h.shape)\n",
    "        h = h.view(h.size(0), -1)                  # torch.Size([32, 384])\n",
    "        # print(h.shape)\n",
    "        if flat_check:\n",
    "            return h.size(1)\n",
    "        h = self.fc1(h)                            # torch.Size([32, 30])\n",
    "        # print(h.shape)\n",
    "        # y = F.softmax(self.fc2(h), dim=1)\n",
    "        y = self.fc2(h)                            # torch.Size([32, 2])\n",
    "        # print(y.shape)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def size(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPvfg4IMAynS"
   },
   "source": [
    "# 4 Train & Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTQcHAEOpH5L"
   },
   "source": [
    "## 4.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "bar_format = '{desc} |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}{postfix}]'\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1662709338437,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "QPnAwK-QAySq"
   },
   "outputs": [],
   "source": [
    "def train_model(mirna_fasta_file, mrna_fasta_file, train_file, model=None, cts_size=30, seed_match='offset-9-mer-m7', level='gene', batch_size=32, epochs=10, save_file=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    if not isinstance(model, deepTarget):\n",
    "        raise ValueError(\"'model' expected <nn.Module 'deepTarget'>, got {}\".format(type(model)))\n",
    "    \n",
    "    print(\"\\n[TRAIN] {}\".format(model.name))\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_file.split('/')[-1] == 'train_set.csv':\n",
    "        train_set = TrainDataset(train_file)\n",
    "    else:\n",
    "        # 实例化\n",
    "        train_set = Dataset(mirna_fasta_file, mrna_fasta_file, train_file, seed_match=seed_match, header=True, train=True)  # return (mirna, mrna), label\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    class_weight = torch.Tensor(compute_class_weight('balanced', classes=np.unique(train_set.labels), y=train_set.labels)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, corrects = 0, 0\n",
    "\n",
    "        with tqdm(train_loader, desc=\"Epoch {}/{}\".format(epoch+1, epochs), bar_format=bar_format) as tqdm_loader:\n",
    "            for i, ((mirna, mrna), label) in enumerate(tqdm_loader):\n",
    "                \n",
    "                mirna, mrna, label = mirna.to(device, dtype=torch.int64), mrna.to(device, dtype=torch.int64), label.to(device)\n",
    "                \n",
    "                outputs = model(mirna, mrna)\n",
    "                loss = criterion(outputs, label)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item() * outputs.size(0)\n",
    "                corrects += (torch.max(outputs, 1)[1] == label).sum().item()\n",
    "                \n",
    "                if (i+1) == len(train_loader):\n",
    "                    tqdm_loader.set_postfix(dict(loss=(epoch_loss/len(train_set)), acc=(corrects/len(train_set))))\n",
    "                else:\n",
    "                    tqdm_loader.set_postfix(loss=loss.item())\n",
    "    \n",
    "    if save_file is None:\n",
    "        time = datetime.now()\n",
    "        save_file = \"{}.pt\".format(time.strftime('%Y%m%d_%H%M%S_weights'))\n",
    "    torch.save(model.state_dict(), save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "47b3949c80c74bcf99defd12c79a1123",
      "310ace27d05a439fbea34750ee1ce672",
      "1d8775a647114f5d9bcf467b8718e901",
      "909e44e6cf7246418a7cfae0704582dc",
      "3572254e352f44128f42983a26e40ec1",
      "0aa10071bde74ae18512f0a04f6a75a0",
      "650b0762b5f0422c8075e9c0835c9782",
      "cc646b7bdecc4d6cbc44b6d68171796d",
      "4ae450109210485ca1d549b7a2584b2c",
      "24dd24d51bc645c2bf91122cc7bcd55f",
      "9017a9796b794a4f9d2d6e46f7869abe"
     ]
    },
    "id": "BSGD0JkvmkB2",
    "outputId": "29348aae-7930-4488-fc56-d852b36c22c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] 2022-09-16 @ 01:16:12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd519447082649f7839a477a110525b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fddf535cc0b4fd0807b9b361603449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bc0578fad047f581c49a0552926a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adc5f496f944b9c82775a723fcece4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d558439ecc413ebae71a25f6a5d8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d61a446dfb42fab81e49904dc624c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4840b8a188048b1b7fa14036713deea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cc52db29554af8a107bc56cc538ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13fe05a6ef54b868161680886d13906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da097dec97b14d2ea5c25d5235482b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10 |          | 0/2045 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FINISH] 2022-09-16 @ 01:32:02 (user time: 0:15:49.759889)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"\\n[START] {}\".format(start_time.strftime('%Y-%m-%d @ %H:%M:%S')))\n",
    "        \n",
    "mirna_fasta_file = 'Data/mirna.fasta'\n",
    "mrna_fasta_file  = 'Data/mrna.fasta'\n",
    "seed_match       = 'offset-9-mer-m7'\n",
    "level            = 'gene'\n",
    "train_file       = 'Data/Train/train_set.csv'\n",
    "weight_file      = 'weights.pt'\n",
    "batch_size       = 32\n",
    "epochs           = 10\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')        \n",
    "\n",
    "model1 = TargetFM1()\n",
    "# model2 = FMTarget()\n",
    "\n",
    "torch.save(model1.state_dict(), weight_file)\n",
    "train_model(mirna_fasta_file, mrna_fasta_file, train_file,\n",
    "             model=model1,\n",
    "             seed_match=seed_match, level=level,\n",
    "             batch_size=batch_size, epochs=epochs,\n",
    "             save_file=weight_file, device=device)\n",
    "        \n",
    "finish_time = datetime.now()\n",
    "print(\"\\n[FINISH] {} (user time: {})\\n\".format(finish_time.now().strftime('%Y-%m-%d @ %H:%M:%S'), (finish_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6jpjkpppORt"
   },
   "source": [
    "## 4.2 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(mirna_fasta_file, mrna_fasta_file, query_file, model=None, weight_file=None, seed_match='offset-9-mer-m7', level='gene', batch_size=32, output_file=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    if not isinstance(model, deepTarget):\n",
    "        raise ValueError(\"'model' expected <nn.Module 'deepTarget'>, got {}\".format(type(model)))\n",
    "    \"\"\"\n",
    "    \n",
    "    if not weight_file.endswith('.pt'):\n",
    "        raise ValueError(\"'weight_file' expected '*.pt', got {}\".format(weight_file))\n",
    "    \n",
    "    model.load_state_dict(torch.load(weight_file))\n",
    "    \n",
    "    test_set = Dataset(mirna_fasta_file, mrna_fasta_file, query_file, seed_match=seed_match, header=True, train=True)    \n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    y_probs = []\n",
    "    y_predicts = []\n",
    "    y_truth = []\n",
    "\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        with tqdm(test_loader, bar_format=bar_format) as tqdm_loader:\n",
    "            for i, ((mirna, mrna), label) in enumerate(tqdm_loader):\n",
    "                mirna, mrna, label = mirna.to(device, dtype=torch.int64), mrna.to(device, dtype=torch.int64), label.to(device)\n",
    "                \n",
    "                outputs = model(mirna, mrna)\n",
    "                _, predicts = torch.max(outputs.data, 1)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                y_probs.extend(probabilities.cpu().numpy()[:, 1])\n",
    "                y_predicts.extend(predicts.cpu().numpy())\n",
    "                y_truth.extend(label.cpu().numpy())\n",
    "\n",
    "                global correct\n",
    "                # print(predicts.cpu().numpy())\n",
    "                # print(label.cpu().numpy())\n",
    "                correct += (predicts == label).sum().item()\n",
    "                \n",
    "        acc = float(correct / len(test_set)) * 100\n",
    "        print(len(test_set))\n",
    "        print(\"acc:\", acc, \"%\")\n",
    "\n",
    "        if output_file is None:\n",
    "            time = datetime.now()\n",
    "            output_file = \"{}.csv\".format(time.strftime('%Y%m%d_%H%M%S_results'))\n",
    "        results = postprocess_result(test_set.dataset, y_probs, y_predicts,\n",
    "                                     seed_match=seed_match, level=level, output_file=output_file)\n",
    "        \n",
    "        # print(results)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "rX8L0iJLjtPe",
    "outputId": "201b7d0e-8198-416d-989c-2aa1fb0dd62c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] 2022-09-16 @ 01:59:03\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2b0c952ad64b2a9bfa27e025161b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/156 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4962\n",
      "acc: 79.32285368802901 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbadbd8baea40b89a5f17d8c7b88f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/171 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5457\n",
      "acc: 84.91845336265347 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbafd88c561241f99607cd285811e017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/162 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5153\n",
      "acc: 82.8449446924122 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136900e017d24051be80ce9791142a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/155 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4956\n",
      "acc: 81.79983857949959 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21a3dead7bb49e4a948d341390804fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/160 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5101\n",
      "acc: 81.96432072142717 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6abf48735b445181e560db79d28d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/169 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5388\n",
      "acc: 79.28730512249443 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc39634a00b343888eae51fcfa2a7c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/160 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5102\n",
      "acc: 79.08663269306155 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10742e53b831425fb48837ee6535e9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/162 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5167\n",
      "acc: 82.11728275595122 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d5ae215df140c5b1bff491ad9dfa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/156 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4986\n",
      "acc: 82.49097472924187 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49db0350714447b883374a3cbaa2e19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/162 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5172\n",
      "acc: 83.33333333333334 %\n",
      "avg =  81.71659396781038 %\n",
      "\n",
      "[FINISH] 2022-09-16 @ 02:01:05 (user time: 0:02:01.979552)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"\\n[START] {}\".format(start_time.strftime('%Y-%m-%d @ %H:%M:%S')))\n",
    "\n",
    "mirna_fasta_file = 'Data/mirna.fasta'\n",
    "mrna_fasta_file  = 'Data/mrna.fasta'\n",
    "query_file       = 'Data/Test/test_split_'\n",
    "model = TargetFM1()\n",
    "weight_file      = 'weights.pt'\n",
    "seed_match = 'offset-9-mer-m7'\n",
    "level = 'gene'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "sum = 0\n",
    "\n",
    "for i in range(10):\n",
    "    correct = 0\n",
    "    query_file1 = query_file + str(i) + '.csv'\n",
    "    acc = predict_result(mirna_fasta_file, mrna_fasta_file, query_file1,\n",
    "                             model=model, weight_file=weight_file,\n",
    "                             seed_match=seed_match, level=level,\n",
    "                             output_file=None, device=device)\n",
    "    sum += acc\n",
    "\n",
    "print(\"avg = \", sum/10.0, \"%\")\n",
    "finish_time = datetime.now()\n",
    "print(\"\\n[FINISH] {} (user time: {})\\n\".format(finish_time.now().strftime('%Y-%m-%d @ %H:%M:%S'), (finish_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn1JhiMKhXJj"
   },
   "source": [
    "# Py3.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(TP, FP, TN, FN):\n",
    "    accuracy = (TP+TN) / (TP+TN+FP+FN)\n",
    "    sensitivity = TP / (TP+FN)\n",
    "    specificity = TN / (TN+FP)\n",
    "    f_measure = 2*TP / (2*TP+FP+FN)\n",
    "    PPV = TP / (TP+FP)\n",
    "    NPV = TN / (TN+FN)\n",
    "\n",
    "    print(\"accuracy:%.2f\" %(accuracy))\n",
    "    print(\"sensitivity:%.2f\" %(sensitivity))\n",
    "    print(\"specifity:%.2f\" %(specificity))\n",
    "    print(\"F-measure:%.2f\" %(F-measure))\n",
    "    print(\"PPV:%.2f\" %(PPV))\n",
    "    print(\"NPV:%.2f\" %(NPV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=0\n",
    "FP=0\n",
    "TN=0\n",
    "FN=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7ghwNBrojZhw"
   },
   "outputs": [],
   "source": [
    "def predict_result2(mirna_fasta_file, mrna_fasta_file, query_file, model=None, weight_file=None, seed_match='offset-9-mer-m7', level='gene', batch_size=32, output_file=None, device='cpu'):\n",
    "    \"\"\"\n",
    "    if not isinstance(model, deepTarget):\n",
    "        raise ValueError(\"'model' expected <nn.Module 'deepTarget'>, got {}\".format(type(model)))\n",
    "    \"\"\"\n",
    "    \n",
    "    if not weight_file.endswith('.pt'):\n",
    "        raise ValueError(\"'weight_file' expected '*.pt', got {}\".format(weight_file))\n",
    "    \n",
    "    global TP\n",
    "    global FP\n",
    "    global TN\n",
    "    global FN\n",
    "    global correct\n",
    "    \n",
    "    model.load_state_dict(torch.load(weight_file))\n",
    "    \n",
    "    test_set = Dataset(mirna_fasta_file, mrna_fasta_file, query_file, seed_match=seed_match, header=True, train=False)    \n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    y_probs = []\n",
    "    y_predicts = []\n",
    "    y_truth = []\n",
    "\n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        with tqdm(test_loader, bar_format=bar_format) as tqdm_loader:\n",
    "            for i, ((mirna, mrna), label) in enumerate(tqdm_loader):\n",
    "                mirna, mrna, label = mirna.to(device, dtype=torch.int64), mrna.to(device, dtype=torch.int64), label.to(device)\n",
    "                \n",
    "                outputs = model(mirna, mrna)\n",
    "                _, predicts = torch.max(outputs.data, 1)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                y_probs.extend(probabilities.cpu().numpy()[:, 1])\n",
    "                y_predicts.extend(predicts.cpu().numpy())\n",
    "                y_truth.extend(label.cpu().numpy())\n",
    "                \n",
    "                correct += (predicts == label).sum().item()\n",
    "\n",
    "                TP += (label==1 and predicts==1).sum().item()\n",
    "                FP += (label==1 and predicts==0).sum().item()\n",
    "                TN += (label==0 and predicts==0).sum().item()\n",
    "                FN += (label==0 and predicts==1).sum().item()\n",
    "                print(TP,FP,TN,FN)\n",
    "                '''\n",
    "                if(y_truth==1 and y_predicts==1):    TP+=1\n",
    "                elif(y_truth==1 and y_predicts==0):  FP+=1\n",
    "                elif(y_truth==0 and y_predicts==0):  TN+=1\n",
    "                elif(y_truth==0 and y_predicts==1):  FN+=1\n",
    "                '''\n",
    "                \n",
    "            metric(TP, FP, TN, FN)\n",
    "        \n",
    "        if output_file is None:\n",
    "            time = datetime.now()\n",
    "            output_file = \"{}.csv\".format(time.strftime('%Y%m%d_%H%M%S_results'))\n",
    "        results = postprocess_result(test_set.dataset, y_probs, y_predicts,\n",
    "                                     seed_match=seed_match, level=level, output_file=output_file)\n",
    "        \n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[START] 2022-09-14 @ 12:02:33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663d0f124a704a6ead9b0fe0419889b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " |          | 0/156 [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15356/2692757358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                          \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                          \u001b[0mseed_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                          output_file=None, device=device)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mfinish_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15356/835393306.py\u001b[0m in \u001b[0;36mpredict_result2\u001b[0;34m(mirna_fasta_file, mrna_fasta_file, query_file, model, weight_file, seed_match, level, batch_size, output_file, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mTP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mFP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mTN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print(\"\\n[START] {}\".format(start_time.strftime('%Y-%m-%d @ %H:%M:%S')))\n",
    "\n",
    "mirna_fasta_file = 'Data/mirna.fasta'\n",
    "mrna_fasta_file  = 'Data/mrna.fasta'\n",
    "query_file       = 'Data/Test/test_split_0.csv'\n",
    "model = deepTarget()\n",
    "weight_file      = 'weights.pt'\n",
    "seed_match = 'offset-9-mer-m7'\n",
    "level = 'gene'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "results = predict_result2(mirna_fasta_file, mrna_fasta_file, query_file,\n",
    "                         model=model, weight_file=weight_file,\n",
    "                         seed_match=seed_match, level=level,\n",
    "                         output_file=None, device=device)\n",
    "        \n",
    "finish_time = datetime.now()\n",
    "print(\"\\n[FINISH] {} (user time: {})\\n\".format(finish_time.now().strftime('%Y-%m-%d @ %H:%M:%S'), (finish_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMTarget1(nn.Module):\n",
    "    def __init__(self, hparams=None, hidden_units=30, input_shape=(2, 30), name_prefix=\"model\"):\n",
    "        super(FMTarget1, self).__init__()\n",
    "\n",
    "        if hparams is None:\n",
    "            filters, kernels = [32, 16, 64, 16], [3, 3, 3, 3]\n",
    "            hparams = HyperParam(filters, kernels)\n",
    "        self.name = \"{}{}\".format(name_prefix, hparams.name_postfix)\n",
    "        self.fc_mi = nn.Linear(640, 32)\n",
    "        self.fc_mr = nn.Linear(640, 32)\n",
    "        \n",
    "        if (isinstance(hparams, HyperParam)) and (len(hparams) == 4):\n",
    "            self.embd1 = nn.Conv1d(4, hparams.f1, kernel_size=hparams.k1, padding=((hparams.k1 - 1) // 2))\n",
    "            \n",
    "            self.conv2 = nn.Conv1d(hparams.f1*2, hparams.f2, kernel_size=hparams.k2)\n",
    "            self.conv3 = nn.Conv1d(hparams.f2, hparams.f3, kernel_size=hparams.k3)\n",
    "            self.conv4 = nn.Conv1d(hparams.f3, hparams.f4, kernel_size=hparams.k4)\n",
    "            \n",
    "            \"\"\" out_features = ((in_length - kernel_size + (2 * padding)) / stride + 1) * out_channels \"\"\"\n",
    "            # flat_features = self.forward(torch.randint(1, 5, input_shape).to(device), torch.randint(1, 5, input_shape).to(device), flat_check=False)\n",
    "            self.fc1 = nn.Linear(1920, hidden_units)\n",
    "            self.fc2 = nn.Linear(hidden_units, 2)\n",
    "        else:\n",
    "            raise ValueError(\"not enough hyperparameters\")\n",
    "    \n",
    "    def forward(self, x_mirna, x_mrna, flat_check=False):\n",
    "        mi_out = get_embed(x_mirna)\n",
    "        mi_out = self.fc_mi(mi_out).transpose(1,2)\n",
    "        mr_out = get_embed(x_mrna)\n",
    "        mr_out = self.fc_mr(mr_out).transpose(1,2)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        h_mirna = F.relu(mi_out)                   # torch.Size([32, 32, 30])\n",
    "        # print(h_mirna.shape)\n",
    "        h_mrna = F.relu(mr_out)                    # torch.Size([32, 32, 30])\n",
    "        # print(h_mrna.shape)\n",
    "        h = torch.cat((h_mirna, h_mrna), dim=1)    # torch.Size([32, 64, 30])\n",
    "        # print(h.shape)\n",
    "\n",
    "        h = h.view(h.size(0), -1)                  # torch.Size([32, 1920])\n",
    "        # print(h.shape)\n",
    "        if flat_check:\n",
    "            return h.size(1)\n",
    "        h = self.fc1(h)                            # torch.Size([32, 30])\n",
    "        # print(h.shape)\n",
    "        # y = F.softmax(self.fc2(h), dim=1)\n",
    "        y = self.fc2(h)                            # torch.Size([32, 2])\n",
    "        # print(y.shape)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def size(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMTarget(nn.Module):\n",
    "    def __init__(self, hparams=None, hidden_units=30, input_shape=(2, 30), name_prefix=\"model\"):\n",
    "        super(FMTarget, self).__init__()\n",
    "\n",
    "        self.fc_mi = nn.Linear(640, 32)\n",
    "        self.fc_mr = nn.Linear(640, 32)\n",
    "        \"\"\" out_features = ((in_length - kernel_size + (2 * padding)) / stride + 1) * out_channels \"\"\"\n",
    "        # flat_features = self.forward(torch.randint(1, 5, input_shape).to(device), torch.randint(1, 5, input_shape).to(device), flat_check=False)\n",
    "        self.fc1 = nn.Linear(1920, hidden_units)\n",
    "        self.fc2 = nn.Linear(hidden_units, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, x_mirna, x_mrna, flat_check=False):\n",
    "        mi_out = get_embed(x_mirna)\n",
    "        mi_out = self.fc_mi(mi_out).transpose(1,2)\n",
    "        mr_out = get_embed(x_mrna)\n",
    "        mr_out = self.fc_mr(mr_out).transpose(1,2)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        h_mirna = F.relu(mi_out)                   # torch.Size([32, 32, 30])\n",
    "        # print(h_mirna.shape)\n",
    "        h_mrna = F.relu(mr_out)                    # torch.Size([32, 32, 30])\n",
    "        # print(h_mrna.shape)\n",
    "        h = torch.cat((h_mirna, h_mrna), dim=1)    # torch.Size([32, 64, 30])\n",
    "        # print(h.shape)\n",
    "\n",
    "        h = h.view(h.size(0), -1)                  # torch.Size([32, 1920])\n",
    "        # print(h.shape)\n",
    "        if flat_check:\n",
    "            return h.size(1)\n",
    "        h = self.fc1(h)                            # torch.Size([32, 30])\n",
    "        # print(h.shape)\n",
    "        # y = F.softmax(self.fc2(h), dim=1)\n",
    "        y = self.fc2(h)                            # torch.Size([32, 2])\n",
    "        # print(y.shape)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def size(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDyZggQqX-gz"
   },
   "outputs": [],
   "source": [
    "  \"\"\"\n",
    "  1. ints_enc[:, 1:5]\n",
    "  2. site_sequence[::-1]  \n",
    "  3. reshape(-1)拉成一行； \n",
    "  4. reshape(-1, 1)拉成1列 reshape(-1, 2)拉成2列\n",
    "  5. np.arange(3): array[0,1,2]\n",
    "  6. categorical[np.arange(n_samples), labels] = 1 :在categorical的前n_samples行按照labels进行索引\n",
    "  7. np.unique(y)：去除数组中的重复数字，并进行排序之后输出\n",
    "  8. records.sort_values(by=['PROBABILITY', 'MIRNA_ID', 'MRNA_ID'], ascending=[False, True, True])  # sort_values()函数原理类似于SQL中的order by，将数据集依照某个字段中的数据进行排序； ascending\t是否按指定列的数组升序排列，默认为True，即升序排列\n",
    "  9. records.to_csv(output_file, index=False, sep='\\t')  # dt.to_csv('C:/Users/think/Desktop/Result.csv',sep='?')#使用?分隔需要保存的数据，如果不写，默认是,\n",
    "  10. zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表\n",
    "  11. for i, (f, k) in enumerate(zip(filters, kernels)):     # get the elements of multiple lists and indexes https://note.nkmk.me/en/python-for-enumerate-zip/\n",
    "  12. getattr(a, 'bar') => 1 # 获取属性 bar 值    /  setattr(a, 'bar', 5) a.bar => 5 # 设置属性 bar 值\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1662655914188,
     "user": {
      "displayName": "白梓琳",
      "userId": "01672587629044439639"
     },
     "user_tz": -480
    },
    "id": "VmzGfBY-PUWA",
    "outputId": "fde413d3-0a86-4ad6-9027-4cf7e10edb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULQW8j7VQjN-"
   },
   "outputs": [],
   "source": [
    "#install python 3.9\n",
    "!sudo apt-get update -y\n",
    "!sudo apt-get install python3.9\n",
    "\n",
    "#change alternatives\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2\n",
    "\n",
    "#check python version\n",
    "!python --version\n",
    "#3.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_9IBiZERxaA"
   },
   "outputs": [],
   "source": [
    "!sudo update-alternatives --config python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoCA-AP6Q7pf"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install python3.9-distutils && wget https://bootstrap.pypa.io/get-pip.py && python get-pip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWnpS0gxSOMC"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A0QOcZXSHUN"
   },
   "outputs": [],
   "source": [
    "!python -m pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoKzW4isSaVT"
   },
   "outputs": [],
   "source": [
    "!pip install rna-fm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Sn1JhiMKhXJj",
    "GnH8zbsQLsZf",
    "0PK0_AN2L1R4"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0aa10071bde74ae18512f0a04f6a75a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f9637eb787249aeb29f655aeecf0b1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a3d4d3e01704a3993da42e97dd2243d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d8775a647114f5d9bcf467b8718e901": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc646b7bdecc4d6cbc44b6d68171796d",
      "max": 16357,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ae450109210485ca1d549b7a2584b2c",
      "value": 2429
     }
    },
    "24dd24d51bc645c2bf91122cc7bcd55f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310ace27d05a439fbea34750ee1ce672": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0aa10071bde74ae18512f0a04f6a75a0",
      "placeholder": "​",
      "style": "IPY_MODEL_650b0762b5f0422c8075e9c0835c9782",
      "value": "Epoch 1/10 "
     }
    },
    "34066fc389e74abda6cc481fe7f744d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e731091a93849edad827ce94a85e7d4",
       "IPY_MODEL_f96c7ee0c6254068a9ee9a212cc173f9",
       "IPY_MODEL_4b8235dcebeb46ed8432bc0a78ebc9e8"
      ],
      "layout": "IPY_MODEL_42abe6644ea9477d9b690397910d5d7a"
     }
    },
    "3572254e352f44128f42983a26e40ec1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42abe6644ea9477d9b690397910d5d7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47b3949c80c74bcf99defd12c79a1123": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_310ace27d05a439fbea34750ee1ce672",
       "IPY_MODEL_1d8775a647114f5d9bcf467b8718e901",
       "IPY_MODEL_909e44e6cf7246418a7cfae0704582dc"
      ],
      "layout": "IPY_MODEL_3572254e352f44128f42983a26e40ec1"
     }
    },
    "4ae450109210485ca1d549b7a2584b2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4b8235dcebeb46ed8432bc0a78ebc9e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7467ca42b614cc7b3e9967fadb18e8c",
      "placeholder": "​",
      "style": "IPY_MODEL_be95e77ac42348a6bc912f6144f89efa",
      "value": " 1.11G/1.11G [11:46&lt;00:00, 1.82MB/s]"
     }
    },
    "650b0762b5f0422c8075e9c0835c9782": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e731091a93849edad827ce94a85e7d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a3d4d3e01704a3993da42e97dd2243d",
      "placeholder": "​",
      "style": "IPY_MODEL_f5b725c12057456aae21366a65ae6226",
      "value": "100%"
     }
    },
    "9017a9796b794a4f9d2d6e46f7869abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "909e44e6cf7246418a7cfae0704582dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24dd24d51bc645c2bf91122cc7bcd55f",
      "placeholder": "​",
      "style": "IPY_MODEL_9017a9796b794a4f9d2d6e46f7869abe",
      "value": " 2429/16357 [49:42&lt;4:38:13, loss=0.116]"
     }
    },
    "be95e77ac42348a6bc912f6144f89efa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc646b7bdecc4d6cbc44b6d68171796d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee093b64aa134309b09faabcd714dc72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5b725c12057456aae21366a65ae6226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7467ca42b614cc7b3e9967fadb18e8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f96c7ee0c6254068a9ee9a212cc173f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee093b64aa134309b09faabcd714dc72",
      "max": 1194424423,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f9637eb787249aeb29f655aeecf0b1f",
      "value": 1194424423
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
